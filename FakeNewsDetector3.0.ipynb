{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23f93ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in c:\\users\\erdem\\anaconda3\\lib\\site-packages (2.0.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\erdem\\anaconda3\\lib\\site-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\erdem\\anaconda3\\lib\\site-packages (from xgboost) (1.11.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import re\n",
    "import string\n",
    "\n",
    "# Import necessary libraries\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# Loading the datasets\n",
    "data_fake = pd.read_csv(\"fake.csv\")\n",
    "data_true = pd.read_csv(\"True.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3b30896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding class labels\n",
    "data_fake[\"class\"] = 0\n",
    "data_true[\"class\"] = 1\n",
    "\n",
    "# Removing last 10 entries for manual testing\n",
    "data_fake_manual_testing = data_fake.tail(10)\n",
    "data_true_manual_testing = data_true.tail(10)\n",
    "data_fake = data_fake.iloc[:-10]\n",
    "data_true = data_true.iloc[:-10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e9bf5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Merging the datasets\n",
    "data_merge = pd.concat([data_fake, data_true], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73428353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping unnecessary columns\n",
    "data = data_merge.drop([\"title\", \"subject\", \"date\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40acc35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffling the data\n",
    "data = data.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e641d421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text preprocessing function\n",
    "def wordopt(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub(\"\\\\W\", \" \", text)\n",
    "    text = re.sub(\"https?://\\S+|www\\S+\", \"\", text)\n",
    "    text = re.sub(\"<.*?>+\", \"\", text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub(\"\\n\", \"\", text)\n",
    "    text = re.sub(\"\\w*\\d\\w*\", \"\", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5a4013f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Applying text preprocessing\n",
    "data['text'] = data['text'].apply(wordopt)\n",
    "\n",
    "# Splitting the data\n",
    "x = data['text']\n",
    "y = data['class']\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8f2d3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Vectorization\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorization = TfidfVectorizer()\n",
    "xv_train = vectorization.fit_transform(x_train)\n",
    "xv_test = vectorization.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63ff1c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Importing necessary libraries\n",
    "from sklearn.linear_model import LogisticRegression, PassiveAggressiveClassifier, Perceptron\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import (GradientBoostingClassifier, RandomForestClassifier, \n",
    "                              AdaBoostClassifier, ExtraTreesClassifier, \n",
    "                              BaggingClassifier, VotingClassifier)\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from xgboost import XGBClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4833af08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression: LogisticRegression(max_iter=1000)\n",
      "Passive Aggressive Classifier: PassiveAggressiveClassifier()\n",
      "Perceptron: Perceptron()\n",
      "Decision Tree: DecisionTreeClassifier()\n",
      "Gradient Boosting: GradientBoostingClassifier()\n",
      "Random Forest: RandomForestClassifier()\n",
      "AdaBoost: AdaBoostClassifier()\n",
      "Extra Trees: ExtraTreesClassifier()\n",
      "Bagging: BaggingClassifier()\n",
      "Voting: VotingClassifier(estimators=[('lr', LogisticRegression(max_iter=1000)),\n",
      "                             ('rf', RandomForestClassifier()),\n",
      "                             ('svc', SVC(probability=True))],\n",
      "                 voting='soft')\n",
      "SVC: SVC()\n",
      "Multinomial NB: MultinomialNB()\n",
      "KNeighbors: KNeighborsClassifier()\n",
      "Linear Discriminant Analysis: LinearDiscriminantAnalysis()\n",
      "Quadratic Discriminant Analysis: QuadraticDiscriminantAnalysis()\n",
      "XGBoost: XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...)\n"
     ]
    }
   ],
   "source": [
    "# List of models to evaluate\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000),  # Added max_iter to ensure convergence\n",
    "    'Passive Aggressive Classifier': PassiveAggressiveClassifier(),\n",
    "    'Perceptron': Perceptron(),\n",
    "    'Decision Tree': DecisionTreeClassifier(),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(),\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    'AdaBoost': AdaBoostClassifier(),\n",
    "    'Extra Trees': ExtraTreesClassifier(),\n",
    "    'Bagging': BaggingClassifier(),\n",
    "    'Voting': VotingClassifier(estimators=[\n",
    "        ('lr', LogisticRegression(max_iter=1000)),  # Added max_iter to ensure convergence\n",
    "        ('rf', RandomForestClassifier()),\n",
    "        ('svc', SVC(probability=True))\n",
    "    ], voting='soft'),\n",
    "    'SVC': SVC(),\n",
    "    'Multinomial NB': MultinomialNB(),\n",
    "    'KNeighbors': KNeighborsClassifier(),\n",
    "    'Linear Discriminant Analysis': LinearDiscriminantAnalysis(),\n",
    "    'Quadratic Discriminant Analysis': QuadraticDiscriminantAnalysis(),\n",
    "    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')  # Added params to avoid warnings\n",
    "}\n",
    "\n",
    "\n",
    "# Display the models\n",
    "for name, model in models.items():\n",
    "    print(f\"{name}: {model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068a91fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression: 0.9859\n",
      "Passive Aggressive Classifier: 0.9935\n",
      "Perceptron: 0.9906\n",
      "Decision Tree: 0.9954\n",
      "Gradient Boosting: 0.9949\n",
      "Random Forest: 0.9898\n",
      "AdaBoost: 0.9956\n",
      "Extra Trees: 0.9840\n",
      "Bagging: 0.9969\n"
     ]
    }
   ],
   "source": [
    "# Evaluating each model\n",
    "results = {}\n",
    "\n",
    "# Evaluate each model\n",
    "for name, model in models.items():\n",
    "    model.fit(xv_train, y_train)  # Train the model\n",
    "    pred = model.predict(xv_test)  # Predict on the test set\n",
    "    accuracy = accuracy_score(y_test, pred)  # Calculate accuracy\n",
    "    results[name] = accuracy  # Store the accuracy in the results dictionary\n",
    "    print(f\"{name}: {accuracy:.4f}\")  # Print the accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af46e15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Selecting the top 5 models\n",
    "top_models = sorted(results.items(), key=lambda item: item[1], reverse=True)[:5]\n",
    "top_model_names = [model[0] for model in top_models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13281ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Creating a Voting Classifier with the top 5 models\n",
    "voting_estimators = [(name, dict(models)[name]) for name in top_model_names]\n",
    "voting_clf = VotingClassifier(estimators=voting_estimators, voting='hard')\n",
    "voting_clf.fit(xv_train, y_train)\n",
    "pred_voting = voting_clf.predict(xv_test)\n",
    "print(\"Voting Classifier Accuracy:\", voting_clf.score(xv_test, y_test))\n",
    "print(classification_report(y_test, pred_voting))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e65cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual testing function\n",
    "def output_label(n):\n",
    "    return 'Fake News' if n == 0 else 'True News'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adf7118",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def manual_testing(news):\n",
    "    testing_news = {\"text\": [news]}\n",
    "    new_def_test = pd.DataFrame(testing_news)\n",
    "    new_def_test['text'] = new_def_test['text'].apply(wordopt)\n",
    "    new_x_test = new_def_test['text']\n",
    "    new_xv_test = vectorization.transform(new_x_test)\n",
    "    \n",
    "    pred_voting = voting_clf.predict(new_xv_test)\n",
    "    \n",
    "    print(f\"Voting Classifier prediction: {output_label(pred_voting[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c935cd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the manual testing function\n",
    "manual_testing(\"Your sample news text goes here\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3967fd58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115d3856",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
